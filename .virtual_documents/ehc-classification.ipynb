








import numpy as np 
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from scipy.stats import shapiro, mstats, jarque_bera
sns.set()

# For pltoly graphs
import plotly.io as pio
pio.renderers.default = "notebook_connected"

from plotly.offline import init_notebook_mode
init_notebook_mode(connected=True)

from plotly.subplots import make_subplots
import plotly.graph_objs as go
import plotly.figure_factory as ff

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler

from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, roc_auc_score, accuracy_score

# To ignore unwanted warnings
import warnings
warnings.filterwarnings('ignore')

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))





data  = pd.read_csv('data-ori.csv')
data.tail(5)


# Set Encoding labels for the target variable 'SORUCE'
data['SOURCE'] = data['SOURCE'].replace({'in': 1, 'out': 0})

# List all the features of the dataset
features = [col for col in data.columns if col != 'SOURCE']

#List the categorical and numerical features
num_feats = [numf for numf in features if data[numf].dtype != object]
cat_feats = [catf for catf in features if data[catf].dtype == object]
print(f'The feature table contains:\n1. {len(num_feats)} numerical feature[s] -', end=' ')
print(*num_feats, sep=', ')
print(f'2. {len(cat_feats)} categorical feature[s] -', end=' ')
print(*cat_feats)








data.info()





# Unique values for each feature and target variables

data.nunique().to_frame('Unqiue Values')





data.describe(include='all')


# Distribution of variables
numeric_columns = data.select_dtypes(include=[np.number]).columns
num_vars = len(numeric_columns)

# Calculate the number of rows needed in the subplot grid (2 columns grid here for more horizontal space)
num_rows = (num_vars + 1) // 2

plt.figure(figsize=(12, num_rows * 4))  # Adjust the figure size based on the number of rows
for i, col in enumerate(numeric_columns):
    plt.subplot(num_rows, 2, i + 1)  # 2 columns
    sns.histplot(data[col], kde=True)
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()








# Assume df_robust is your DataFrame with robust scaling applied to the necessary columns
numeric_cols = ['HAEMATOCRIT', 'HAEMOGLOBINS', 'ERYTHROCYTE', 'LEUCOCYTE', 'THROMBOCYTE', 'MCH', 'MCHC', 'MCV', 'AGE']

# Initialize a dictionary to store test results
normality_results = {}

# Determine the grid size for subplots
n_cols = 3  # Number of columns in the grid
n_rows = (len(numeric_cols) + n_cols - 1) // n_cols  # Calculate rows needed, rounded up

# Create a figure with subplots
fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 5 * n_rows))
axes = axes.flatten()  # Flatten the axes array for easier iteration

# Loop through each column to plot
for i, column in enumerate(numeric_cols):
    # Perform Jarque-Bera test
    stat, p = jarque_bera(data[column])
    normality_results[column] = {'Statistic': stat, 'p-value': p}

    # Plot Q-Q plot in the respective subplot
    sm.qqplot(data[column], line='45', fit=True, ax=axes[i])
    axes[i].set_title(f'Q-Q plot for {column}')

# Adjust layout to prevent overlap
plt.tight_layout()

# Hide any unused subplots if the number of plots isn't a perfect fit for the grid
for ax in axes[len(numeric_cols):]:
    ax.set_visible(False)

# Show the plot grid
plt.show()
# Convert results to a DataFrame for better visualization
normality_test_df = pd.DataFrame(normality_results).T  # Transpose to have columns as headers
print(normality_test_df)








# Log Transformation (add 1 to shift all values away from zero)
data['LEUCOCYTE_log'] = np.log1p(data['LEUCOCYTE'])

# Winsorizing the data (limiting extreme values to the 95th percentile)
data['LEUCOCYTE_winsorized'] = mstats.winsorize(data['LEUCOCYTE'], limits=[0, 0.10])


# Log Transformation (add 1 to shift all values away from zero)
data['THROMBOCYTE_log'] = np.log1p(data['THROMBOCYTE'])

# Winsorizing the data (limiting extreme values to the 95th percentile)
data['THROMBOCYTE_winsorized'] = mstats.winsorize(data['THROMBOCYTE'], limits=[0, 0.05])


data


# Select numeric columns (excluding categorical columns like 'SEX' and 'SOURCE')
# numeric_cols = ['HAEMATOCRIT', 'HAEMOGLOBINS', 'ERYTHROCYTE', 'LEUCOCYTE_winsorized', 'THROMBOCYTE_winsorized', 'MCH', 'MCHC', 'MCV', 'AGE']
numeric_cols = ['HAEMATOCRIT', 'HAEMOGLOBINS', 'ERYTHROCYTE', 'LEUCOCYTE_log', 'THROMBOCYTE_log', 'MCH', 'MCHC', 'MCV', 'AGE']

# # Min-Max Scaling
# min_max_scaler = MinMaxScaler()
# df_min_max = data.copy()
# df_min_max[numeric_cols] = min_max_scaler.fit_transform(data[numeric_cols])

# Z-score Normalization
standard_scaler = StandardScaler()
df_standard = data.copy()
df_standard[numeric_cols] = standard_scaler.fit_transform(data[numeric_cols])

# Robust Scaling
robust_scaler = RobustScaler()
df_robust = data.copy()
df_robust[numeric_cols] = robust_scaler.fit_transform(data[numeric_cols])


# Distribution of variables
num_vars = len(numeric_cols)

# Calculate the number of rows needed in the subplot grid (2 columns grid here for more horizontal space)
num_rows = (num_vars + 1) // 2

plt.figure(figsize=(12, num_rows * 4))  # Adjust the figure size based on the number of rows
for i, col in enumerate(numeric_cols):
    plt.subplot(num_rows, 2, i + 1)  # 2 columns
    sns.histplot(df_robust[col], kde=True)
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()


data


df_standard


# Initialize a dictionary to store test results
normality_results = {}

# Determine the grid size for subplots
n_cols = 3  # Number of columns in the grid
n_rows = (len(numeric_cols) + n_cols - 1) // n_cols  # Calculate rows needed, rounded up

# Create a figure with subplots
fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 5 * n_rows))
axes = axes.flatten()  # Flatten the axes array for easier iteration

# Loop through each column to plot
for i, column in enumerate(numeric_cols):
    # Perform Jarque-Bera test
    stat, p = jarque_bera(df_robust[column])
    normality_results[column] = {'Statistic': stat, 'p-value': p}

    # Plot Q-Q plot in the respective subplot
    sm.qqplot(df_robust[column], line='45', fit=True, ax=axes[i])
    axes[i].set_title(f'Q-Q plot for {column}')

# Adjust layout to prevent overlap
plt.tight_layout()

# Hide any unused subplots if the number of plots isn't a perfect fit for the grid
for ax in axes[len(numeric_cols):]:
    ax.set_visible(False)

# Show the plot grid
plt.show()

# Convert results to a DataFrame for better visualization
normality_test_df = pd.DataFrame(normality_results).T  # Transpose to have columns as headers

print(normality_test_df)








import plotly.express as px

fig = px.histogram(data, x='SOURCE')
fig.show()


sns.displot(data['SOURCE'], kind='ecdf', stat='proportion');








data[num_feats].describe()





for column in num_feats:
    fig = make_subplots(rows=1, cols=2)
    
    #Box plot
    fig.add_trace(
    go.Box(y=data[column], name=column),
    row =1, col=1)
    
    # KDE plot
    hist_data = [data[column]]
    group_labels = [column]
    distplot = ff.create_distplot(hist_data, group_labels, show_hist=False, show_rug=False)
    for trace in distplot.data:
        fig.add_trace(trace, row=1, col=2)
    fig.show()


data.loc[data['LEUCOCYTE']==76.6]


data.loc[data['THROMBOCYTE']==1183]








fig, ax = plt.subplots(1,2, figsize=(15,7))
sns.countplot(data, x='SEX', ax=ax[0])
plt.pie(data['SEX'].value_counts(), autopct='%0.2f%%', labels=['M', 'F'])
plt.show()


data['SEX'].value_counts()





fig, axs = plt.subplots(5, 2, figsize=(14,22))
axes = [ax for rows in axs for ax in rows]
for idx, feat in enumerate(data[num_feats]):
    plot = sns.kdeplot(data=data, x=feat, hue='SOURCE', multiple='stack', ax=axes[idx])
sns.countplot(data=data, x='SEX', hue='SOURCE', ax=axs[4,1]);





corr = data[num_feats].corr(method='spearman')
triu_corr = np.triu(corr)
sns.heatmap(corr, annot=True, mask=triu_corr, cmap='BrBG', cbar=False)
plt.show()








## Train test split

# Feature set
X = data[features]
# Target set 
y = data['SOURCE']

# Splitting data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)

# train and test datasets dimensions
X_train.shape, X_test.shape, y_train.shape, y_test.shape


# Label Encding

# Encode binary classes for the 'M' and 'F' categorical values

X_train.SEX.replace({'F':0, 'M': 1}, inplace=True)
X_test.SEX.replace({'F':0, 'M': 1}, inplace=True)


# Feature Scaling

# Based on the distribution and outliers observed in the univariate analysis we can resort to 
# using a MinMaxScaler from sklearn

# minmax = MinMaxScaler(feature_range=(0,1))
scaler = MinMaxScaler(feature_range=(0,1))

X_train[num_feats] = scaler.fit_transform(X_train[num_feats]) #fit and transform the train set
X_test[num_feats] = scaler.transform(X_test[num_feats]) #transform the test test











continuous_vars = data.select_dtypes(include=[float, int])
correlation_matrix = continuous_vars.corr()
print(correlation_matrix)








# Chi-Square Test of Independence for SEX and SOURCE
contingency_table = pd.crosstab(data['SEX'], data['SOURCE'])
chi2, p, dof, expected = stats.chi2_contingency(contingency_table)
print(f"Chi-Square Statistic: {chi2}, p-value: {p}")





# Chi-Square Test of Independence for SEX and SOURCE
contingency_table = pd.crosstab(data['HAEMATOCRIT'], data['HAEMOGLOBINS'])
chi2, p, dof, expected = stats.chi2_contingency(contingency_table)
print(f"Chi-Square Statistic: {chi2}, p-value: {p}")





# Chi-Square Test of Independence for SEX and SOURCE
contingency_table = pd.crosstab(data['MCH'], data['MCV'])
chi2, p, dof, expected = stats.chi2_contingency(contingency_table)
print(f"Chi-Square Statistic: {chi2}, p-value: {p}")





#Columns dropped based on correlation of data that would render the feature values redundant

X_train.drop(['HAEMATOCRIT','MCH'], axis=1, inplace=True)
X_test.drop(['HAEMATOCRIT','MCH'], axis=1, inplace=True)





# Define the models
model1 = sm.OLS.from_formula("SOURCE ~ 1", data=data)
model2 = sm.OLS.from_formula("SOURCE ~ AGE + SEX + HAEMOGLOBINS", data=data)

# Fit the models
result1 = model1.fit()
result2 = model2.fit()

# Perform the likelihood ratio test
lrt_stat = -2 * (result1.llf - result2.llf)
lrt_pval = stats.chi2.sf(lrt_stat, df=result2.df_model - result1.df_model)

print("Likelihood Ratio Test:")
print("Test Statistic:", lrt_stat)
print("p-value:", lrt_pval)








# Define the models
model1 = sm.OLS.from_formula("SOURCE ~ AGE + SEX + HAEMOGLOBINS", data=data)
model2 = sm.OLS.from_formula("SOURCE ~ HAEMOGLOBINS + ERYTHROCYTE + LEUCOCYTE + THROMBOCYTE + MCHC + MCV + AGE + SEX", data=data)

# Fit the models
result1 = model1.fit()
result2 = model2.fit()

# Perform the likelihood ratio test
lrt_stat = -2 * (result1.llf - result2.llf)
lrt_pval = stats.chi2.sf(lrt_stat, df=result2.df_model - result1.df_model)

print("Likelihood Ratio Test:")
print("Test Statistic:", lrt_stat)
print("p-value:", lrt_pval)


result1.summary()


result2.summary()














logreg = LogisticRegression(random_state=42)

#Fit the model
logreg.fit(X_train, y_train)


logreg_precision, logreg_recall, logreg_f2score, logreg_support = precision_recall_fscore_support(y_test, logreg.predict(X_test), beta=2)
logreg_accuracy = accuracy_score(y_test, logreg.predict(X_test))
logreg_rocauc = roc_auc_score(y_test, logreg.predict(X_test))
logreg_confusion_matrix = confusion_matrix(y_test, logreg.predict(X_test))


print(f'---Metrics---\n accuracy: {logreg_accuracy*100:.0f}%\n precision: {logreg_precision}\n\
 recall: {logreg_recall}\n f2_score: {logreg_f2score}\n\
 ROC AUC Score: {logreg_rocauc:0.2f}\n\n\
 confusion matrix:\n {logreg_confusion_matrix}')





svm =SVC(kernel='rbf', C=14, random_state=42)
svm.fit(X_train,y_train)


svm_precision, svm_recall, svm_f2score, svm_support = precision_recall_fscore_support(y_test, svm.predict(X_test), beta=2)
svm_accuracy = accuracy_score(y_test, svm.predict(X_test))
svm_rocauc = roc_auc_score(y_test, svm.predict(X_test))
svm_confusion_matrix = confusion_matrix(y_test, svm.predict(X_test))


print(f'---Metrics---\n accuracy: {svm_accuracy*100:.1f}%\n precision: {svm_precision}\n\
 recall: {svm_recall}\n f2_score: {svm_f2score}\n\
 ROC AUC Score: {svm_rocauc:0.2f}\n\n\
 confusion matrix:\n {svm_confusion_matrix}')





# Training the K-NN model on the Training set
knn = KNeighborsClassifier(n_neighbors=9, metric='minkowski', p=2)
knn.fit(X_train, y_train)


knn_precision, knn_recall, knn_f2score, knn_support = precision_recall_fscore_support(y_test, knn.predict(X_test), beta=2)
knn_accuracy = accuracy_score(y_test, knn.predict(X_test))
knn_rocauc = roc_auc_score(y_test, knn.predict(X_test))
knn_confusion_matrix = confusion_matrix(y_test, knn.predict(X_test))


print(f'---Metrics---\n accuracy: {knn_accuracy*100:.1f}%\n precision: {knn_precision}\n\
 recall: {knn_recall}\n f2_score: {knn_f2score}\n\
 ROC AUC Score: {knn_rocauc:0.2f}\n\n\
 confusion matrix:\n {knn_confusion_matrix}')





random_forest = RandomForestClassifier(n_estimators=40, random_state=42)

# Fit the model
random_forest.fit(X_train, y_train)


random_forest_precision, random_forest_recall, random_forest_f2score, random_forest_support = precision_recall_fscore_support(y_test, random_forest.predict(X_test), beta=2)
random_forest_accuracy = accuracy_score(y_test, random_forest.predict(X_test))
random_forest_rocauc = roc_auc_score(y_test, random_forest.predict(X_test))
random_forest_confusion_matrix = confusion_matrix(y_test, random_forest.predict(X_test))


print(f'---Metrics---\n accuracy: {random_forest_accuracy*100:.1f}%\n precision: {random_forest_precision}\n\
 recall: {random_forest_recall}\n f2_score: {random_forest_f2score}\n\
 ROC AUC Score: {random_forest_rocauc:0.2f}\n\n\
 confusion matrix:\n {random_forest_confusion_matrix}')








# Hyperparameters
param_grid = {'C': [1, 5, 10, 20, 40, 50, 100],
              'kernel': ['rbf'],
              'degree':[1, 2, 4, 6]
             } 

# Random search for best hyperparameters
search = GridSearchCV(SVC(random_state=42),
                         param_grid,
                         scoring='accuracy',
                         cv=3,
                         verbose=1)

search.fit(X_train, y_train)

# Best parameters for Support vector classifier
search.best_params_

best_svc = search.best_estimator_


# Fit the model
best_svc.fit(X_train, y_train)

svm_precision_opt, svm_recall_opt, svm_f2score_opt, svm_support_opt = precision_recall_fscore_support(y_test, best_svc.predict(X_test), beta=2)
svm_accuracy_opt = accuracy_score(y_test, best_svc.predict(X_test))


print(f'---Metrics---\n accuracy: {svm_accuracy_opt*100:.1f}%\n precision: {svm_precision_opt}\n\
 recall: {svm_recall_opt}\n f2_score: {svm_f2score_opt}')


best_svc.coef_





# GRID SEARCH
# Define the parameter values that should be searched
param_grid = {'n_neighbors': [5, 7],
              'metric': ['cosine'],
              'weights': ['distance'],
              'algorithm': ['auto'],
              'leaf_size': [10]}

# Create a base model
knn = KNeighborsClassifier()

# Instantiate the grid search model
grid_search = GridSearchCV(knn, param_grid, cv=3, verbose=1, n_jobs=-1)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Check the best parameters
print(grid_search.best_params_)

# Use the best estimator for our predictions on test
best_knn = grid_search.best_estimator_


# Fit the model
best_knn.fit(X_train, y_train)

knn_precision_opt, knn_recall_opt, knn_f2score_opt, knn_support_opt = precision_recall_fscore_support(y_test, best_knn.predict(X_test), beta=2)
knn_accuracy_opt = accuracy_score(y_test, best_knn.predict(X_test))


print(f'---Metrics---\n accuracy: {knn_accuracy_opt*100:.1f}%\n precision: {knn_precision_opt}\n\
 recall: {knn_recall_opt}\n f2_score: {knn_f2score_opt}')





# Grid Search
# param_grid = {
#     'n_estimators': [100, 200, 350, 500],  # More options for number of trees
#     'max_features': ['sqrt', 'log2'],      # Two types of features to consider
#     'criterion': ['gini', 'entropy', 'log_loss'],  # Tree split criteria
#     'min_samples_leaf': [1, 2, 4],         # Minimum number of samples at leaf node
#     'min_samples_split': [2, 5, 7],        # Minimum number of samples to split
#     'max_depth': [None, 10, 20, 30],       # Maximum depth of the tree
#     'bootstrap': [True, False]             # Whether bootstrap samples are used
# }
param_grid = {'n_estimators': [350], 
              'max_features': ['log2'],
              'criterion': ['gini', 'entropy', 'log_loss'],
             'min_samples_leaf': [1],
             'min_samples_split': [7],
              'max_depth' : [None],
             'bootstrap': [False]}

# Create a base model
random_forest = RandomForestClassifier(random_state=42)

# Instantiate the grid search model
grid_search = GridSearchCV(random_forest, param_grid, cv=3, verbose=1)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Check the best parameters
print(grid_search.best_params_)

# You can now use the best estimator for further predictions
best_rf = grid_search.best_estimator_


# Fit the model
best_rf.fit(X_train, y_train)

random_forest_precision_opt, random_forest_recall_opt, random_forest_f2score_opt, random_forest_support_opt = precision_recall_fscore_support(y_test, best_rf.predict(X_test), beta=2)
random_forest_accuracy_opt = accuracy_score(y_test, best_rf.predict(X_test))

print(f'---Metrics---\n accuracy: {random_forest_accuracy_opt*100:.1f}%\n precision: {random_forest_precision_opt}\n\
 recall: {random_forest_recall_opt}\n f2_score: {random_forest_f2score_opt}')





best_rf_feature_importance = pd.DataFrame({'features': best_rf.feature_names_in_, 'importance': best_rf.feature_importances_})
best_rf_feature_importance.sort_values(
    by='importance', 
    ascending=True, 
    inplace=True
)
plt.figure(figsize=(10, 6))
plt.barh(best_rf_feature_importance['features'], best_rf_feature_importance['importance'])
plt.xlabel('Importance')
plt.title('Feature Importance')
plt.show()


from sklearn import tree

# Extract one of the trees from the forest
chosen_tree = best_rf.estimators_[0]

# Set the figure size
plt.figure(figsize=(20,10))

# Plot the decision tree
tree.plot_tree(chosen_tree, 
               filled=True, 
               feature_names=best_rf.feature_names_in_, 
               class_names=best_rf.classes_.astype(str),
               rounded=True,
               fontsize=10)
plt.show()





# Create a function to plot confusion matrix and classification metrics
def plot_cm(cm, precision, recall, f2score, accuracy, title):
    plt.figure(figsize=(10,5))
    
    plt.subplot(1, 2, 1)
    sns.heatmap(cm, annot=True, fmt=".0f", linewidths=.5, square = True, cmap = 'Blues');
    plt.ylabel('Actual label');
    plt.xlabel('Predicted label');
    
    plt.subplot(1, 2, 2)
    metrics = [precision, recall, f2score, accuracy]
    metric_names = ['Precision', 'Recall', 'F2 Score', 'Accuracy']
    sns.barplot(x=metric_names, y=metrics)
    plt.ylim(0, 1)
    
    plt.suptitle(title, size = 15)


# Compute average metrics
svm_precision_opt_avg = np.mean(svm_precision_opt)
svm_recall_opt_avg = np.mean(svm_recall_opt)
svm_f2score_opt_avg = np.mean(svm_f2score_opt)

knn_precision_opt_avg = np.mean(knn_precision_opt)
knn_recall_opt_avg = np.mean(knn_recall_opt)
knn_f2score_opt_avg = np.mean(knn_f2score_opt)

random_forest_precision_opt_avg = np.mean(random_forest_precision_opt)
random_forest_recall_opt_avg = np.mean(random_forest_recall_opt)
random_forest_f2score_opt_avg = np.mean(random_forest_f2score_opt)

# Compute confusion matrices
cm_svm = confusion_matrix(y_test, best_svc.predict(X_test))
cm_knn = confusion_matrix(y_test, best_knn.predict(X_test))
cm_rf = confusion_matrix(y_test, best_rf.predict(X_test))

# Plot confusion matrices and classification metrics
plot_cm(cm_svm, svm_precision_opt_avg, svm_recall_opt_avg, svm_f2score_opt_avg, svm_accuracy_opt, 'Support Vector Machine')
plot_cm(cm_knn, knn_precision_opt_avg, knn_recall_opt_avg, knn_f2score_opt_avg, knn_accuracy_opt, 'K-Nearest Neighbor')
plot_cm(cm_rf, random_forest_precision_opt_avg, random_forest_recall_opt_avg, random_forest_f2score_opt_avg, random_forest_accuracy_opt, 'Random Forest Classifier')



y_test_pred = best_svc.predict(X_test)
y_test_pred = pd.DataFrame({'predicted' : y_test_pred})
X_test.reset_index(drop=True, inplace=True)
X_y_test_pred = pd.concat([X_test, y_test_pred], axis=1)
X_y_test_pred.head()


# Setting up the figure with subplots
fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(12, 16))  # Adjust the figsize to fit your display
axes = axes.flatten()  # Flatten the axes array for easy iteration

# Box Plots for each feature
features = ['HAEMOGLOBINS', 'ERYTHROCYTE', 'LEUCOCYTE', 'THROMBOCYTE', 'MCHC', 'MCV', 'AGE']
for i, feature in enumerate(features):
    sns.boxplot(x='predicted', y=feature, data=X_y_test_pred, palette='bwr', ax=axes[i])
    axes[i].set_title(f'Distribution of {feature} by Prediction Outcome')

# If there's an empty subplot, hide it
if len(axes) > len(features):
    for ax in axes[len(features):]:
        ax.axis('off')

plt.tight_layout()
plt.show()















